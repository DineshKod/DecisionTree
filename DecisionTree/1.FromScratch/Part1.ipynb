{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bf73913-81b0-479e-baa4-02c171da97cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import csv\n",
    "# Note: please don't add any new package, you should solve this problem using only the packages above.\n",
    "# However, importing the Python standard library is allowed: https://docs.python.org/3/library/\n",
    "#-------------------------------------------------------------------------\n",
    "'''\n",
    "    Part 1: Decision Tree (with Discrete Attributes) -- 60 points --\n",
    "    In this problem, you will implement the decision tree method for classification problems.\n",
    "    You could test the correctness of your code by typing `pytest -v test1.py` in the terminal.\n",
    "'''\n",
    "\n",
    "#-----------------------------------------------\n",
    "class Node:\n",
    "    '''\n",
    "        Decision Tree Node (with discrete attributes)\n",
    "        Inputs: \n",
    "            X: the data instances in the node, a numpy matrix of shape p by n.\n",
    "               Each element can be int/float/string.\n",
    "               Here n is the number data instances in the node, p is the number of attributes.\n",
    "            Y: the class labels, a numpy array of length n.\n",
    "               Each element can be int/float/string.\n",
    "            i: the index of the attribute being tested in the node, an integer scalar \n",
    "            C: the dictionary of attribute values and children nodes. \n",
    "               Each (key, value) pair represents an attribute value and its corresponding child node.\n",
    "            isleaf: whether or not this node is a leaf node, a boolean scalar\n",
    "            p: the label to be predicted on the node (i.e., most common label in the node).\n",
    "    '''\n",
    "    def __init__(self,X,Y, i=None,C=None, isleaf= False,p=None):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.i = i\n",
    "        self.C= C\n",
    "        self.isleaf = isleaf\n",
    "        self.p = p\n",
    "\n",
    "#-----------------------------------------------\n",
    "class Tree(object):\n",
    "    '''\n",
    "        Decision Tree (with discrete attributes). \n",
    "        We are using ID3(Iterative Dichotomiser 3) algorithm. So this decision tree is also called ID3.\n",
    "    '''\n",
    "    #--------------------------\n",
    "    @staticmethod\n",
    "    def entropy(Y):\n",
    "        '''\n",
    "            Compute the entropy of a list of values.\n",
    "            Input:\n",
    "                Y: a list of values, a numpy array of int/float/string values.\n",
    "            Output:\n",
    "                e: the entropy of the list of values, a float scalar\n",
    "            Hint: you could use collections.Counter.\n",
    "        '''\n",
    "        #########################################\n",
    "        ## INSERT YOUR CODE HERE\n",
    "             \n",
    "        counter = Counter(Y)  #Count the number of each label. For example: yes: 5, no: 3\n",
    "        total = len(Y)     #Count total number of labels in the dataset. For example: 8\n",
    "        e = 0\n",
    "        for count in counter.values():\n",
    "            p = count/total #probability\n",
    "            e -= p * math.log2(p) #entropy\n",
    "\n",
    "        #########################################\n",
    "        return e \n",
    "    \n",
    "    \n",
    "            \n",
    "    #--------------------------\n",
    "    @staticmethod\n",
    "    def conditional_entropy(Y,X):\n",
    "        '''\n",
    "            Compute the conditional entropy of y given x. The conditional entropy H(Y|X) means average entropy of children nodes, given attribute X. Refer to https://en.wikipedia.org/wiki/Information_gain_in_decision_trees\n",
    "            Input:\n",
    "                X: a list of values , a numpy array of int/float/string values. The size of the array means the number of instances/examples. X contains each instance's attribute value. \n",
    "                Y: a list of values, a numpy array of int/float/string values. Y contains each instance's corresponding target label. For example X[0]'s target label is Y[0]\n",
    "            Output:\n",
    "                ce: the conditional entropy of y given x, a float scalar\n",
    "        '''\n",
    "        #########################################\n",
    "        ## INSERT YOUR CODE HERE\n",
    "\n",
    "        \n",
    "        unique_values = np.unique(X)  #Get all the unique attributes. For example: \n",
    "        total = len(Y)\n",
    "        ce = 0\n",
    "        for val in unique_values:\n",
    "            subset_Y = Y[X == val]\n",
    "            p = len(subset_Y)/total\n",
    "            subset_Y_entropy = Tree.entropy(subset_Y)\n",
    "            ce += p * subset_Y_entropy\n",
    "        \n",
    " \n",
    "        #########################################\n",
    "        return ce \n",
    "    \n",
    "    \n",
    "    \n",
    "    #--------------------------\n",
    "    @staticmethod\n",
    "    def information_gain(Y,X):\n",
    "        '''\n",
    "            Compute the information gain of y after spliting over attribute x\n",
    "            InfoGain(Y,X) = H(Y) - H(Y|X) \n",
    "            Input:\n",
    "                X: a list of values, a numpy array of int/float/string values.\n",
    "                Y: a list of values, a numpy array of int/float/string values.\n",
    "            Output:\n",
    "                g: the information gain of y after spliting over x, a float scalar\n",
    "        '''\n",
    "        #########################################\n",
    "        ## INSERT YOUR CODE HERE\n",
    "        ## IG = parent entropy - average childnode entropy\n",
    "        \n",
    "        H_Y = Tree.entropy(Y)\n",
    "        H_Y_given_X = Tree.conditional_entropy(Y, X)\n",
    "        g = H_Y - H_Y_given_X\n",
    " \n",
    "        #########################################\n",
    "        return g\n",
    "\n",
    "\n",
    "    #--------------------------\n",
    "    @staticmethod\n",
    "    def best_attribute(X,Y):\n",
    "        '''\n",
    "            Find the best attribute to split the node. \n",
    "            Here we use information gain to evaluate the attributes. \n",
    "            If there is a tie in the best attributes, select the one with the smallest index.\n",
    "            Input:\n",
    "                X: the feature matrix, a numpy matrix of shape p by n. \n",
    "                   Each element can be int/float/string.\n",
    "                   Here n is the number data instances in the node, p is the number of attributes.\n",
    "                Y: the class labels, a numpy array of length n. Each element can be int/float/string.\n",
    "            Output:\n",
    "                i: the index of the attribute to split, an integer scalar\n",
    "        '''\n",
    "        #########################################\n",
    "        ## INSERT YOUR CODE HERE\n",
    "        \n",
    "        num_attributes = X.shape[0]\n",
    "        best_gain = -1\n",
    "        i = -1\n",
    "        \n",
    "        for j in range(num_attributes):\n",
    "            gain = Tree.information_gain(Y, X[j])\n",
    "            if gain > best_gain: \n",
    "                best_gain = gain\n",
    "                i = j\n",
    "            elif gain == best_gain and j < i: \n",
    "                i = j \n",
    " \n",
    "        #########################################\n",
    "        return i\n",
    "\n",
    "        \n",
    "    #--------------------------\n",
    "    @staticmethod\n",
    "    def split(X,Y,i):\n",
    "        '''\n",
    "            Split the node based upon the i-th attribute.\n",
    "            (1) split the matrix X based upon the values in i-th attribute\n",
    "            (2) split the labels Y based upon the values in i-th attribute\n",
    "            (3) build children nodes by assigning a submatrix of X and Y to each node\n",
    "            (4) build the dictionary to combine each  value in the i-th attribute with a child node.\n",
    "    \n",
    "            Input:\n",
    "                X: the feature matrix, a numpy matrix of shape p by n.\n",
    "                   Each element can be int/float/string.\n",
    "                   Here n is the number data instances in the node, p is the number of attributes.\n",
    "                Y: the class labels, a numpy array of length n.\n",
    "                   Each element can be int/float/string.\n",
    "                i: the index of the attribute to split, an integer scalar\n",
    "            Output:\n",
    "                C: the dictionary of attribute values and children nodes. \n",
    "                   Each (key, value) pair represents an attribute value and its corresponding child node.\n",
    "        '''\n",
    "        #########################################\n",
    "        ## INSERT YOUR CODE HERE\n",
    "        C = {}\n",
    "        \n",
    "        \n",
    "        unique_vals = np.unique(X[i, :])\n",
    "\n",
    "        for value in unique_vals:\n",
    "            indices = np.where(X[i] == value)\n",
    "\n",
    "            X_sub = X[:, indices[0]]\n",
    "            Y_sub = Y[indices]\n",
    "\n",
    "            child_node = Node(X_sub, Y_sub)\n",
    "\n",
    "            C[value] = child_node\n",
    "\n",
    "\n",
    "        #########################################\n",
    "        return C\n",
    "\n",
    "    #--------------------------\n",
    "    @staticmethod\n",
    "    def stop1(Y):\n",
    "        '''\n",
    "            Test condition 1 (stop splitting): whether or not all the instances have the same label. \n",
    "    \n",
    "            Input:\n",
    "                Y: the class labels, a numpy array of length n.\n",
    "                   Each element can be int/float/string.\n",
    "            Output:\n",
    "                s: whether or not Condition 1 holds, a boolean scalar. \n",
    "                True if all labels are the same. Otherwise, false.\n",
    "        '''\n",
    "        #########################################\n",
    "        ## INSERT YOUR CODE HERE\n",
    "        \n",
    "        s = np.all(Y == Y[0])       \n",
    "        \n",
    "        #########################################\n",
    "        return s\n",
    "    \n",
    "    #--------------------------\n",
    "    @staticmethod\n",
    "    def stop2(X):\n",
    "        '''\n",
    "            Test condition 2 (stop splitting): whether or not all the instances have the same attribute values. \n",
    "            Input:\n",
    "                X: the feature matrix, a numpy matrix of shape p by n.\n",
    "                   Each element can be int/float/string.\n",
    "                   Here n is the number data instances in the node, p is the number of attributes.\n",
    "            Output:\n",
    "                s: whether or not Conidtion 2 holds, a boolean scalar. \n",
    "        '''\n",
    "        #########################################\n",
    "        ## INSERT YOUR CODE HERE\n",
    "        \n",
    "        \n",
    "        \n",
    "        if np.all(X == X[:, [0]], axis=1).all():\n",
    "            s = True\n",
    "        else:\n",
    "            s = False\n",
    "    \n",
    "        #########################################\n",
    "        return s\n",
    "    \n",
    "            \n",
    "    #--------------------------\n",
    "    @staticmethod\n",
    "    def most_common(Y):\n",
    "        '''\n",
    "            Get the most-common label from the list Y. \n",
    "            Input:\n",
    "                Y: the class labels, a numpy array of length n.\n",
    "                   Each element can be int/float/string.\n",
    "                   Here n is the number data instances in the node.\n",
    "            Output:\n",
    "                y: the most common label, a scalar, can be int/float/string.\n",
    "        '''\n",
    "        #########################################\n",
    "        ## INSERT YOUR CODE HERE\n",
    "        Y = np.array(Y)\n",
    "        \n",
    "        counter = Counter(Y)\n",
    "        y = counter.most_common(1)[0][0]\n",
    " \n",
    "        #########################################\n",
    "        return y\n",
    "    \n",
    "    \n",
    "    \n",
    "    #--------------------------\n",
    "    @staticmethod\n",
    "    def build_tree(t):\n",
    "        '''\n",
    "            Recursively build tree nodes.\n",
    "            Input:\n",
    "                t: a node of the decision tree, without the subtree built.\n",
    "                t.X: the feature matrix, a numpy float matrix of shape p by n.\n",
    "                   Each element can be int/float/string.\n",
    "                    Here n is the number data instances, p is the number of attributes.\n",
    "                t.Y: the class labels of the instances in the node, a numpy array of length n.\n",
    "                t.C: the dictionary of attribute values and children nodes. \n",
    "                   Each (key, value) pair represents an attribute value and its corresponding child node.\n",
    "        '''\n",
    "        #########################################\n",
    "        ## INSERT YOUR CODE HERE\n",
    "        t.p = Tree.most_common(t.Y)\n",
    "        if Tree.stop1(t.Y) == False and Tree.stop2(t.X) == False:\n",
    "            t.i = Tree.best_attribute(t.X, t.Y)\n",
    "            t.C = Tree.split(t.X, t.Y, t.i) \n",
    "\n",
    "            for val in t.C.values():\n",
    "                Tree.build_tree(val)\n",
    "        else:\n",
    "            t.isleaf=True\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    " \n",
    "        #########################################\n",
    "    \n",
    "    \n",
    "    #--------------------------\n",
    "    @staticmethod\n",
    "    def train(X, Y):\n",
    "        '''\n",
    "            Given a training set, train a decision tree. \n",
    "            Input:\n",
    "                X: the feature matrix, a numpy matrix of shape p by n.\n",
    "                   Each element can be int/float/string.\n",
    "                   Here n is the number data instances in the training set, p is the number of attributes.\n",
    "                Y: the class labels, a numpy array of length n.\n",
    "                   Each element can be int/float/string.\n",
    "            Output:\n",
    "                t: the root of the tree.\n",
    "        '''\n",
    "        #########################################\n",
    "        ## INSERT YOUR CODE HERE\n",
    "        \n",
    "        \n",
    "        t = Node(X, Y)\n",
    "        Tree.build_tree(t)\n",
    " \n",
    "        #########################################\n",
    "        return t\n",
    "    \n",
    "    \n",
    "    \n",
    "    #--------------------------\n",
    "    @staticmethod\n",
    "    def inference(t,x):\n",
    "        '''\n",
    "            Given a decision tree and one data instance, infer the label of the instance recursively. \n",
    "            Input:\n",
    "                t: the root of the tree.\n",
    "                x: the attribute vector, a numpy vectr of shape p.\n",
    "                   Each attribute value can be int/float/string.\n",
    "            Output:\n",
    "                y: the class labels, a numpy array of length n.\n",
    "                   Each element can be int/float/string.\n",
    "        '''\n",
    "        #########################################\n",
    "        ## INSERT YOUR CODE HERE\n",
    "\n",
    "        if t.isleaf:\n",
    "            y = t.p\n",
    "            return y\n",
    "\n",
    "\n",
    "        attribute_value = x[t.i]\n",
    "        if attribute_value in t.C:\n",
    "            return Tree.inference(t.C[attribute_value], x)\n",
    "        else:\n",
    "            y = t.p\n",
    "\n",
    " \n",
    "        #########################################\n",
    "        return y\n",
    "    \n",
    "    #--------------------------\n",
    "    @staticmethod\n",
    "    def predict(t,X):\n",
    "        '''\n",
    "            Given a decision tree and a dataset, predict the labels on the dataset. \n",
    "            Input:\n",
    "                t: the root of the tree.\n",
    "                X: the feature matrix, a numpy matrix of shape p by n.\n",
    "                   Each element can be int/float/string.\n",
    "                   Here n is the number data instances in the dataset, p is the number of attributes.\n",
    "            Output:\n",
    "                Y: the class labels, a numpy array of length n.\n",
    "                   Each element can be int/float/string.\n",
    "        '''\n",
    "        #########################################\n",
    "        ## INSERT YOUR CODE HERE\n",
    "\n",
    "               \n",
    "        Y = np.array([Tree.inference(t, X[:, i]) for i in range(X.shape[1])])\n",
    "\n",
    "        #########################################\n",
    "        return Y\n",
    "\n",
    "\n",
    "\n",
    "    #--------------------------\n",
    "    @staticmethod\n",
    "    def load_dataset(filename = 'data1.csv'):\n",
    "        '''\n",
    "            Load dataset 1 from the CSV file: 'data1.csv'. \n",
    "            The first row of the file is the header (including the names of the attributes)\n",
    "            In the remaining rows, each row represents one data instance.\n",
    "            The first column of the file is the label to be predicted.\n",
    "            In remaining columns, each column represents an attribute.\n",
    "            Input:\n",
    "                filename: the filename of the dataset, a string.\n",
    "            Output:\n",
    "                X: the feature matrix, a numpy matrix of shape p by n.\n",
    "                   Each element can be int/float/string.\n",
    "                   Here n is the number data instances in the dataset, p is the number of attributes.\n",
    "                Y: the class labels, a numpy array of length n.\n",
    "                   Each element can be int/float/string.\n",
    "        '''\n",
    "        #########################################\n",
    "        ## INSERT YOUR CODE HERE\n",
    "        with open(filename, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            header = next(reader)\n",
    "            data = list(reader)\n",
    "        data = np.array(data)\n",
    "\n",
    "        X = data[:, 1:].T\n",
    "        Y = data[:, 0]\n",
    " \n",
    "        #########################################\n",
    "        return X,Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3deea316-5ada-49d5-886d-69e1e22c03f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 100.0%\n",
      "Test Accuracy: 71.42857142857143%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(Y_pred, Y_true):\n",
    "    correct = np.sum(Y_pred == Y_true)\n",
    "    total = len(Y_true)\n",
    "    acc = correct / total\n",
    "    return acc\n",
    "\n",
    "\n",
    "X, Y = Tree.load_dataset('data1.csv')\n",
    "\n",
    "# Train and test split\n",
    "Xtrain, Ytrain = X[:, ::2], Y[::2]\n",
    "Xtest, Ytest = X[:, 1::2], Y[1::2]\n",
    "# Building the decision tree\n",
    "tree = Tree.train(Xtrain, Ytrain)\n",
    "# Predicting on the training and test sets\n",
    "Ytrain_pred = Tree.predict(tree, Xtrain)\n",
    "Ytest_pred = Tree.predict(tree, Xtest)\n",
    "# Evaluate the performance\n",
    "train_accuracy = accuracy(Ytrain_pred, Ytrain)\n",
    "test_accuracy = accuracy(Ytest_pred, Ytest)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy * 100}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fc9face-b834-4fce-a0d0-364319a7efe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain if we split Weather: 0.24674981977443933\n",
      "Information Gain if we split Temperature: 0.31493685137324057\n",
      "Information Gain if we split Random Feature: 0.9402859586706311\n",
      "Best attribute to split on: 0\n",
      "Attribute: Overcast, Nodes: X=[['Overcast' 'Overcast' 'Overcast' 'Overcast']\n",
      " ['Hot' 'Cool' 'Mild' 'Hot']\n",
      " ['High' 'Normal' 'High' 'Normal']], Y=['Yes' 'Yes' 'Yes' 'Yes']\n",
      "Attribute: Rain, Nodes: X=[['Rain' 'Rain' 'Rain' 'Rain' 'Rain']\n",
      " ['Mild' 'Cool' 'Cool' 'Mild' 'Mild']\n",
      " ['High' 'Normal' 'Normal' 'Normal' 'High']], Y=['Yes' 'Yes' 'No' 'Yes' 'No']\n",
      "Attribute: Sunny, Nodes: X=[['Sunny' 'Sunny' 'Sunny' 'Sunny' 'Sunny']\n",
      " ['Hot' 'Hot' 'Mild' 'Cool' 'Mild']\n",
      " ['High' 'High' 'High' 'Normal' 'Normal']], Y=['No' 'No' 'No' 'Yes' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "        ## Testing...\n",
    "Weather = ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', \n",
    "           'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', \n",
    "           'Sunny', 'Overcast', 'Overcast', 'Rain'] \n",
    "\n",
    "Temperature = ['High', 'High', 'High', 'Low', 'Low', \n",
    "           'High', 'High', 'High', 'Low', 'Medium', \n",
    "           'Low', 'Medium', 'Medium', 'Medium'] \n",
    "\n",
    "Random_feature = [0,0,9,9,9,0,9,0,9,9,9,9,9,0]\n",
    "\n",
    "Play_Tennis = ['No', 'No', 'Yes', 'Yes', 'Yes', \n",
    "               'No', 'Yes', 'No', 'Yes', 'Yes',  \n",
    "               'Yes', 'Yes', 'Yes', 'No']\n",
    "\n",
    "ig_weather = Tree.information_gain(Play_Tennis, Weather)\n",
    "print(f'Information Gain if we split Weather: {ig_weather}')\n",
    "\n",
    "ig_temp = Tree.information_gain(Play_Tennis, Temperature)\n",
    "print(f'Information Gain if we split Temperature: {ig_temp}')\n",
    "\n",
    "ig_rf = Tree.information_gain(Play_Tennis, Random_feature)\n",
    "print(f'Information Gain if we split Random Feature: {ig_rf}')\n",
    "\n",
    "\n",
    "\n",
    "features = np.array([['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],\n",
    "              ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],\n",
    "              ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High']])\n",
    "\n",
    "target = np.array(['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No'])\n",
    "\n",
    "best_attr = Tree.best_attribute(features,target)\n",
    "print(f'Best attribute to split on: {best_attr}')\n",
    "\n",
    "split_test = Tree.split(features, target, best_attr)\n",
    "for val, node in split_test.items():\n",
    "    print(f'Attribute: {val}, Nodes: X={node.X}, Y={node.Y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e94c35ec-ce21-48d1-a087-15a6868fcf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tree.train(features, target)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d1e33-7af3-4a87-a323-bd0c52f88e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
